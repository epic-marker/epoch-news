### 台学者：AI法制化应“以人为本” 盘点社会冲击现况

---

#### [首页](../../../..?n14022387) &nbsp;|&nbsp; [评论](../../../../../epoch-comment?n14022387) &nbsp;|&nbsp; [专题](../../../../../epoch-special?n14022387) &nbsp;|&nbsp; [禁闻](../../../../../epoch-news?n14022387) &nbsp;|&nbsp; [禁书](../../../../../books?n14022387) &nbsp;|&nbsp; [翻墙](https://github.com/gfw-breaker/nogfw/blob/master/README.md?n14022387)


<div class="column" id="artbody" itemprop="articleBody">
 <!-- article content begin -->
 <p>
  【大纪元2023年06月25日讯】（大纪元记者侯骏霖台湾台北报导）谈到人工智慧（AI），不免让人联想到2004年威尔史密斯（Will Smith）主演的《机械公敌》（I,
  <br/>
  Robot），当时就示警AI偏误，可能导致一场机器人和人类的战争。学者强调，AI站在风口浪尖，台湾将AI逐步法制化，必须“以人为本”，并评估对社会层面的冲击。
 </p>
 <figure aria-describedby="caption-14022389" class="wp-caption aligncenter" id="14022389" style="width: 500px">
  <ok href=" https://i.epochtimes.com/assets/uploads/2023/06/id14022389-621076-450x300.jpg" rel="noreferrer noopener" target="_blank">
   <img alt="图为AI程式ChatGPT的标志。" src="https://i.epochtimes.com/assets/uploads/2023/06/id14022389-621076-450x300.jpg"/>
  </ok>
  <br/><figcaption class="wp-caption-text" id="caption-14022389">
   图为AI程式ChatGPT的标志。（MARCO BERTORELLO/AFP via Getty Images）
  </figcaption><br/>
 </figure><br/>
 <p>
  《机械公敌》提到的机器人三定律，科幻迷一定不陌生，一是机器人不能伤害人类，或坐视人类受到伤害；二是机器人必须服从人类命令，除非该命令与定律一冲突；三是机器人必须保护自身存在，除非与定律一、定律二冲突。
 </p>
 <p>
  但是机器人原本与人类和平共处，却开始出现伤害人类的行动，主要是随着三定律的演算偏误，最终让机器人得出扭曲结论，认为人类之间的战争会让人类自我毁灭，所以它们的行动是为了保护人类，这就是一种算法偏误。
 </p>
 <p>
  除了算法偏误问题，AI技术或产品本身就可能有善恶两种用途，举例来说，AI人脸识别技术能用于数位装置保护，或是出入办公大楼的通行证，但也可能被用来监控特定人士的行踪；无人机可以作为农业用途帮助播种，但也可能成为自动化武器。因此必须有AI
  <ok href="https://www.epochtimes.com/gb/tag/%E4%BC%A6%E7%90%86.html">
   伦理
  </ok>
  规范为基底，作为法制化推动前提。
 </p>
 <h4>
  AI若被滥用 后果恐难料
 </h4>
 <p>
  人工智慧法律国际研究基金会执行长、高雄大学财法系讲座教授张丽卿告诉《大纪元时报》，全球掀起一波AI浪潮，各种应用如雨后春笋般冒出，但是也给社会带来新风险，例如AI的安全性没有获得保障，可能发展成自动化的武器系统，就会对人类社会构成威胁。
 </p>
 <p>
  她认为，因为AI技术被滥用，很多后果难以预测，企业以AI取代人类的工作、进而裁员，显而易见的影响就是对
  <ok href="https://www.epochtimes.com/gb/tag/%E5%8A%B3%E5%8A%A8%E5%B8%82%E5%9C%BA.html">
   劳动市场
  </ok>
  的冲击。若不早点完善法制，等风险失控造成人民权益受害时，就可能有一波反扑，所以政府应盘点AI对社会的冲击。
 </p>
 <p>
  张丽卿直言，世界各国讨论AI
  <ok href="https://www.epochtimes.com/gb/tag/%E4%BC%A6%E7%90%86.html">
   伦理
  </ok>
  规范都是“以人为本”，且若涉及资安层面要更小心，建议
  <ok href="https://www.epochtimes.com/gb/tag/%E5%85%AC%E5%8A%A1%E6%9C%BA%E5%85%B3.html">
   公务机关
  </ok>
  应优先订出AI系统使用规范，例如连到外网、有机密资料外流疑虑的部分，必须禁用。
 </p>
 <p>
  针对该规范，国科会将以资料保护与治理、风险控管、安全性、智慧财产权、个人隐私、执行公务的机敏性与专业性等六大面向，并参考国外政府对
  <ok href="https://www.epochtimes.com/gb/tag/%E5%85%AC%E5%8A%A1%E6%9C%BA%E5%85%B3.html">
   公务机关
  </ok>
  使用生成式AI的审慎因应做法，来研拟台版规定。
 </p>
 <h4>
  AI伦理规范有五大原则
 </h4>
 <p>
  张丽卿提到AI伦理规范的五大原则，首先是安全性，整体AI系统需要确认是安全、可控，不会导致人类生命、身体、自由或财产陷入风险与危机。
 </p>
 <p>
  其次，AI需具公正客观性，不应有任何种族、性别、宗教、国籍等不同因素，对用户产生偏见与歧视；第三，系统要有透明性，也就是可解释性，一旦发生事故或出现法律责任，可追溯源头问责。
 </p>
 <p>
  第四为隐私性，应遵守个资保户相关规范，且要明确告知AI如何搜集用户数据；第五是永续性，意即不能对环境社会产生重大负面影响，甚至要评估AI对
  <ok href="https://www.epochtimes.com/gb/tag/%E5%8A%B3%E5%8A%A8%E5%B8%82%E5%9C%BA.html">
   劳动市场
  </ok>
  的冲击、保护公平交易的秩序等。
 </p>
 <p>
  若翻开其他国家AI法制化进程，张丽卿说，日本经济发展产业省2022年发布实践AI的治理指导方针，主要是针对企业发展，提供一个AI发展指引，并透过民间与政府的力量，制定相关伦理准则，作为未来立法方向的参照。
 </p>
 <p>
  德国AI伦理准则很早就融入各项领域的法律规范中，2017、2021年修正《道路交通安全法》及《保险法》，订定有关自驾车的法律规定。她认为，待台湾AI
  <ok href="https://www.epochtimes.com/gb/tag/%E5%9F%BA%E6%9C%AC%E6%B3%95.html">
   基本法
  </ok>
  草案9月推出后，各国AI纳管进度，都是各部会可以思考及借鉴的依据。
 </p>
 <h4>
  欧盟拟立法  开全球第一枪
 </h4>
 <p>
  欧盟6月14日通过名为“人工智慧法案”的立法草案，这是西方第一部全面的AI法规，预期法案最终版本将于今年底出炉，但已能看出AI监管的轮廓。
 </p>
 <p>
  首先是生成式AI系统的制造商，必须采取保护措施，以防止它生成非法内容；其次，企业须标记AI生成的内容，防止AI被滥用在假讯息传播。
 </p>
 <p>
  第三，制造商须提供摘要，揭露训练AI模型的资料来源；第四，若出版商、内容创作者的作品，被AI工具用作生成内容的材料时，制造商必须提供创作者分润方案。
 </p>
 <h4>
  高额罚款 或导致ChatGPT脱欧
 </h4>
 <p>
  若未能遵守，恐处以高达3千万欧元（约新台币10亿元）或公司全球年收入6%~7%罚款，对谷歌（Google）及微软（Microsoft）等科技公司来说，罚款可能上看数十亿美元。尽管该草案获得内容创作者或出版商响应，但外界认为，这可能导致ChatGPT最终“脱欧”。
 </p>
 <p>
  此外，值得注意的是，草案内容也明文规定，AI工具的风险若无法被欧洲议会接受，将被禁用，例如中共政府实施的社会评分系统、影响选民投票的工具或推荐算法，以及将民众行为、社经地位、种族分类的AI系统等。◇
 </p>
 <p>
  责任编辑：郑桦
 </p>
 <!-- article content end -->
</div>


<img src='http://gfw-breaker.win/epoch-news/pages/ncid1349361/n14022387.md' width='0px' height='0px'/>

---

原文链接（需翻墙）：https://www.epochtimes.com/gb/23/6/25/n14022387.htm